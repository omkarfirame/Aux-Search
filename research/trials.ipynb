{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Omkar\\Anaconda3\\envs\\pyenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from AuxSearch.components.PDFTextExtractor import PDFConverter\n",
    "from AuxSearch.components.TextChunkerEmbedder import TextProcessor\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from AuxSearch.constants import *\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "pdfconverter = PDFConverter()\n",
    "textprocessor = TextProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def user_input(question):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "    # new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "    vectorstore = FAISS.load_local(\"faiss_index\",embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_type = \"similarity\",k=3)\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "        Answer the question to the point without description. If answer not found in context reply \"Answer Not found\"\\n\\n\n",
    "        Context:\\n {context}?\\n\n",
    "        Question: \\n{question}\\n\n",
    "\n",
    "        Answer:\n",
    "    \"\"\"\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                                temperature=0.3)\n",
    "    prompt = PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])\n",
    "\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\":retriever,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    rag_chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"list down the skills\"\n",
    "pdf = \"Omkar_Firame_Resume.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = pdfconverter.pdf_to_text(pdf)\n",
    "text_chunks = textprocessor.get_chunks(raw_text)\n",
    "textprocessor.get_vector_store(text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Omkar\\nFirame\\n+919404350615\\n|\\nE-Mail\\n|\\nLinkedin\\n|\\nGitHub\\nSUMMAR Y:\\nData\\nScientist\\nwith\\nstrong\\nMachine\\nLearning\\nfoundation\\nand\\n3+\\nyears\\nof\\nexperience\\nin\\ndeveloping\\nrobust\\nML\\nmodels\\nusing\\npredictive\\ndata\\nmodeling,\\nanalyze\\ndata\\nto\\ndeliver\\ninsights\\nand\\nimplement\\naction-oriented\\nsolutions\\nto\\ncomplex\\nbusiness\\nproblems.\\nTECHNICAL\\nSKILLS:\\nPython,\\nR,\\nMachine\\nLearning,\\nDeep\\nLearning,\\nPyTorch,\\nIguazio,\\nKubeFlow\\nPipeline,\\nAzure\\nDatabricks,\\nDask,\\nWeb\\nScraping\\n(Selenium),\\nGit,\\nEXPERIENCE:\\nData\\nScientist\\n-',\n",
       " 'Pipeline,\\nAzure\\nDatabricks,\\nDask,\\nWeb\\nScraping\\n(Selenium),\\nGit,\\nEXPERIENCE:\\nData\\nScientist\\n-\\nWynum\\nAutomation\\nServices\\nPvt.\\nLtd\\n|\\nApril\\n2023\\n-\\nPresent\\n●\\nManaged\\na\\nproject\\nimplementing\\nan\\ninformation\\nretrieval\\nsystem\\nfor\\nextracting\\nrelevant\\npress\\nreleases\\nfrom\\nonline\\nsources.\\n●\\nApplied\\nadvanced\\ndata\\npreprocessing\\ntechniques\\nto\\ncleanse\\nand\\nstructure\\ncontent,\\nenhancing\\ncategorization\\nand\\nanalytical\\ncapabilities.\\n●\\nDeveloped\\ncustom\\nweb\\nscrapers\\nusing\\nPython\\n(Beautiful\\nSoup,\\nRequests)\\nfor\\nprecise',\n",
       " 'capabilities.\\n●\\nDeveloped\\ncustom\\nweb\\nscrapers\\nusing\\nPython\\n(Beautiful\\nSoup,\\nRequests)\\nfor\\nprecise\\nand\\ntargeted\\ncontent\\nextraction.\\nInitiated\\nan\\nindependent\\nproject\\nto\\nautomate\\nDOCX\\nfile\\ngeneration\\nfrom\\ntext\\ndata,\\nshowcasing\\nproactive\\nand\\ninnovative\\nproblem-solving.\\nData\\nScientist\\n-\\nTata\\nConsultancy\\nServices\\n|\\nFeb\\n2021\\n-\\nMarch\\n2023\\n●\\nSpearheaded\\nend-to-end\\ndata\\ninitiatives\\ninvolving\\ndata\\nextraction,\\ntransformation,\\nand\\nloading\\nfrom\\nAzure\\nData\\nLake\\nfor\\nstreamlined\\nprocesses\\nin\\nData\\nPreparation,',\n",
       " 'transformation,\\nand\\nloading\\nfrom\\nAzure\\nData\\nLake\\nfor\\nstreamlined\\nprocesses\\nin\\nData\\nPreparation,\\nCleaning,\\nand\\nModel\\nBuilding.\\n●\\nEngineered\\nand\\nimplemented\\nrobust\\npipelines,\\nutilizing\\nKubeFlow ,\\na\\ncustom\\nManual\\npipeline,\\nand\\nthe\\nmlrun\\nMLOps\\norchestration\\nframework\\non\\nthe\\nIguazio\\nplatform,\\nensuring\\nseamless\\nautomation\\non\\na\\nscheduled\\nbasis.\\n●\\nOrchestrated\\ndata\\ntransformations\\nto\\nmeet\\nAPI\\nrequirements,\\nsystematically\\nschematizing\\ndatasets\\nfor\\nenhanced\\ncompatibility\\nand\\nintegration.\\nApplied\\nDask',\n",
       " 'systematically\\nschematizing\\ndatasets\\nfor\\nenhanced\\ncompatibility\\nand\\nintegration.\\nApplied\\nDask\\ndistributed\\ncomputing\\nfor\\nefficient\\nhandling\\nof\\nlarge-scale\\ndatasets,\\noptimizing\\ndata\\nprocessing\\nand\\nanalysis\\nworkflows.\\nIntern\\n-\\nThoughtworks\\nTechnologies\\nIndia\\nPvt.\\nLtd\\n|\\nJan\\n2020\\n-\\nSept\\n-\\n2020\\n●\\nPioneered\\nthe\\napplication\\nof\\nmachine\\nlearning\\nin\\nthe\\nmedical\\ndomain,\\nfocusing\\non\\npredicting\\ndrug\\nproperties\\nbased\\non\\nchemical\\ncomposition.\\n●\\nAutomated\\nraw\\ndata\\nprocessing\\nwith\\nPython\\nscripts\\nand\\nkey',\n",
       " 'based\\non\\nchemical\\ncomposition.\\n●\\nAutomated\\nraw\\ndata\\nprocessing\\nwith\\nPython\\nscripts\\nand\\nkey\\nlibraries\\n(chemoPy ,\\nPyBioMed,\\nModred),\\nstreamlining\\ndata\\npreparation\\nfor\\npredictive\\nmodeling.\\n●\\nDemonstrated\\nexpertise\\nin\\ndrug\\ntoxicity\\nprediction,\\nsecuring\\n6th\\nposition\\nin\\nthe\\nTox21\\nchallenge\\n(2014-15)\\nwith\\nan\\noutstanding\\naverage\\nAUC\\nscore\\nof\\n0.8347.\\nPROJECTS:\\nText\\nSummarization\\n-\\n[\\nLink\\n]\\n●\\nDeveloped\\na\\nText\\nSummarization\\npersonal\\nproject,\\nleveraging\\nStreamlit\\nfor\\nan\\nintuitive\\nuser\\ninterface\\nthat',\n",
       " \"a\\nText\\nSummarization\\npersonal\\nproject,\\nleveraging\\nStreamlit\\nfor\\nan\\nintuitive\\nuser\\ninterface\\nthat\\nenhances\\nuser\\nexperience\\nand\\ninteraction.\\n●\\nImplemented\\na\\nmodularized\\nstructure,\\nintegrating\\nthe\\nT5\\nsmall\\nmodel\\nfor\\nefficient\\nand\\neffective\\ntext\\nsummarization,\\nshowcasing\\na\\nsystematic\\nand\\norganized\\napproach\\nto\\nproject\\ndevelopment.\\nEDUCA TION\\n-\\nM.TECH\\n-\\nMathematical\\nModeling\\nAnd\\nSimulation\\n-\\nCentre\\nfor\\nModelling\\nand\\nSimulation,\\nSPPU,\\nPune\\n|\\n2020\\nB.E\\n-\\nMechanical\\nEngineering\\n-\\nBharti\\nVidyapeeth's\",\n",
       " \"for\\nModelling\\nand\\nSimulation,\\nSPPU,\\nPune\\n|\\n2020\\nB.E\\n-\\nMechanical\\nEngineering\\n-\\nBharti\\nVidyapeeth's\\nCollege\\nof\\nEngineering,\\nKolhapur\\n|\\n|\\n2016\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "# new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "vectorstore = FAISS.load_local(\"faiss_index\",embeddings)\n",
    "retriever = vectorstore.as_retriever(search_type = \"similarity\",k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000263806CDEB0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"\n",
    "    Answer the question to the point without description. If answer not found in context reply \"Answer Not found\"\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "\"\"\"\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                            temperature=0.6)\n",
    "prompt = PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000263806CDEB0>),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| PromptTemplate(input_variables=['context', 'question'], template='\\n    Answer the question to the point without description. If answer not found in context reply \"Answer Not found\"\\n\\n\\n    Context:\\n {context}?\\n\\n    Question: \\n{question}\\n\\n\\n    Answer:\\n')\n",
       "| ChatGoogleGenerativeAI(model='gemini-pro', temperature=0.6, client= genai.GenerativeModel(\n",
       "     model_name='models/gemini-pro',\n",
       "     generation_config={}.\n",
       "     safety_settings={}\n",
       "  ))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Python\\n- Beautiful Soup\\n- Requests\\n- Azure Data Lake\\n- Azure Databricks\\n- Dask\\n- Web Scraping (Selenium)\\n- Git\\n- Machine Learning\\n- Chemical Composition\\n- Python Scripts'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st.set_page_config(\"Search In PDF\")\n",
    "st.header(\" Serach In PDF \")\n",
    "\n",
    "pdf_docs = st.file_uploader(\"Upload your PDF Files and Click on the Submit Button\", accept_multiple_files=False)\n",
    "user_question = st.text_input(\"Question\")\n",
    "\n",
    "if user_question:\n",
    "    \n",
    "    if st.button(\"Submit & Process\"):\n",
    "        with st.spinner(\"Processing...\"):\n",
    "            raw_text = pdfconverter.pdf_to_text(pdf_docs)\n",
    "            text_chunks = textprocessor.get_chunks(raw_text)\n",
    "            textprocessor.get_vector_store(text_chunks)\n",
    "            user_input(user_question)\n",
    "            st.success(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed code\n",
    "\n",
    "def user_input():\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "    # new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "    vectorstore = FAISS.load_local(\"faiss_index\",embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_type = \"similarity\",k=1)\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "        Answer the question to the point without description. If answer not found in context reply \"Answer Not found\"\\n\\n\n",
    "        Context:\\n {context}?\\n\n",
    "        Question: \\n{question}\\n\n",
    "\n",
    "        Answer:\n",
    "    \"\"\"\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                                temperature=0.6)\n",
    "    prompt = PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])\n",
    "\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\":retriever,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "    )\n",
    "    return rag_chain\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
